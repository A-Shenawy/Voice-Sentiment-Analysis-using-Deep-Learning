# Voice-Sentiment-Analysis
<img src="https://github.com/A-Shenawy/Voice-Sentiment-Analysis/blob/main/VSA.PNG" width="100%" style="background-size: cover;">

ğŸš€ Voice Sentiment Analysis Using Deep Learning

ğŸ“Œ Project Overview
This project aims to classify emotions from voice recordings using deep learning techniques. Human speech carries rich emotional cues, and analyzing these can enhance human-computer interaction, mental health monitoring, and customer experience analysis. The model is trained on the EYASE dataset, which contains Arabic voice samples labeled with emotions: happy, sad, angry, and neutral.

ğŸ¯ Objectives
Develop a CNN-based deep learning model to classify emotions from speech.
Preprocess and augment voice data to improve model generalization.
Extract Mel-Spectrogram features for deep learning-based classification.
Evaluate model performance using accuracy, F1-score, precision, and recall.

ğŸ“Š Dataset Details
Total audio files: 579
Sentiment classes:
Happy: 132
Sad: 147
Angry: 150
Neutral: 150
Average sample rate: 44.1 KHz
Average audio duration: 2.33 sec
Data augmentation techniques applied: Noise addition, pitch shifting
Final dataset after augmentation: 1,626 audio samples

ğŸ› ï¸ Methodology

1ï¸âƒ£ Data Preprocessing & Augmentation
Padding & truncating: Standardizing audio length.
Data augmentation:
Noise addition â†’ Simulates real-world background noise.
Pitch shifting â†’ Enhances model robustness to voice variations.
2ï¸âƒ£ Feature Extraction
Mel-Spectrogram transformation: Converts raw audio into spectrogram images.
Normalization & down-sampling: Reducing sample rate for efficient processing.
3ï¸âƒ£ Model Architecture (CNN-Based Approach)
Input: Mel-Spectrogram images.
Convolutional Layers (Conv2D) â†’ Extracts spatial features.
MaxPooling Layers â†’ Reduces dimensionality.
Dropout Layers â†’ Prevents overfitting.
Flatten Layer â†’ Converts 2D features to 1D.
Dense Layers â†’ Classifies the emotions.
4ï¸âƒ£ Training & Validation
Train-test split: 80% training, 20% testing.
Cross-validation: 5-Fold K-Fold validation to ensure robustness.

ğŸ“ˆ Results & Performance

Initial Model Accuracy: 86.76%
Confusion Matrix Analysis:
High precision & recall for Angry and Sad classes.
Slight misclassification in Neutral vs. Happy classes.
K-Fold Cross-Validation Scores:
Average Accuracy: 87.7%
Average F1 Score: 87.63%
Average Recall: 87.7%
Average Precision: 87.92%

ğŸ“Œ Future Improvements

Optimize model architecture for better generalization.
Explore transformer-based models (e.g., Wav2Vec2, Whisper) for speech processing.
Integrate emotion detection with real-time applications (e.g., chatbots, virtual assistants).

ğŸ“¦ Installation & Usage

ğŸ”§ Prerequisites

Python 3.x
TensorFlow / PyTorch
Librosa (for audio processing)
NumPy, Pandas, Matplotlib
Jupyter Notebook (optional)
