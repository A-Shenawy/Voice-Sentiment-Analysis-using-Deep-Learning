# Voice-Sentiment-Analysis
<img src="https://github.com/A-Shenawy/Voice-Sentiment-Analysis/blob/main/VSA.PNG" width="100%" style="background-size: cover;">

ğŸš€ Voice Sentiment Analysis Using Deep Learning
ğŸ“Œ Project Overview
This project aims to classify emotions from voice recordings using deep learning techniques. Human speech carries rich emotional cues, and analyzing these can enhance human-computer interaction, mental health monitoring, and customer experience analysis. The model is trained on the EYASE dataset, which contains Arabic voice samples labeled with emotions: happy, sad, angry, and neutral.

ğŸ¯ Objectives
Develop a CNN-based deep learning model to classify emotions from speech.
Preprocess and augment voice data to improve model generalization.
Extract Mel-Spectrogram features for deep learning-based classification.
Evaluate model performance using accuracy, F1-score, precision, and recall.

ğŸ“Š Dataset Details
Total audio files: 579
Sentiment classes:
Happy: 132
Sad: 147
Angry: 150
Neutral: 150
Average sample rate: 44.1 KHz
Average audio duration: 2.33 sec
Data augmentation techniques applied: Noise addition, pitch shifting
Final dataset after augmentation: 1,626 audio samples

ğŸ› ï¸ Methodology
1ï¸âƒ£ Data Preprocessing & Augmentation
Padding & truncating: Standardizing audio length.
Data augmentation:
Noise addition â†’ Simulates real-world background noise.
Pitch shifting â†’ Enhances model robustness to voice variations.
2ï¸âƒ£ Feature Extraction
Mel-Spectrogram transformation: Converts raw audio into spectrogram images.
Normalization & down-sampling: Reducing sample rate for efficient processing.
3ï¸âƒ£ Model Architecture (CNN-Based Approach)
Input: Mel-Spectrogram images.
Convolutional Layers (Conv2D) â†’ Extracts spatial features.
MaxPooling Layers â†’ Reduces dimensionality.
Dropout Layers â†’ Prevents overfitting.
Flatten Layer â†’ Converts 2D features to 1D.
Dense Layers â†’ Classifies the emotions.
4ï¸âƒ£ Training & Validation
Train-test split: 80% training, 20% testing.
Cross-validation: 5-Fold K-Fold validation to ensure robustness.
ğŸ“ˆ Results & Performance
Initial Model Accuracy: 86.76%
Confusion Matrix Analysis:
High precision & recall for Angry and Sad classes.
Slight misclassification in Neutral vs. Happy classes.
K-Fold Cross-Validation Scores:
Average Accuracy: 87.7%
Average F1 Score: 87.63%
Average Recall: 87.7%
Average Precision: 87.92%

ğŸ“Œ Future Improvements
Optimize model architecture for better generalization.
Explore transformer-based models (e.g., Wav2Vec2, Whisper) for speech processing.
Integrate emotion detection with real-time applications (e.g., chatbots, virtual assistants).
ğŸ“¦ Installation & Usage
ğŸ”§ Prerequisites
Make sure you have the following installed:

Python 3.x
TensorFlow / PyTorch
Librosa (for audio processing)
NumPy, Pandas, Matplotlib
Jupyter Notebook (optional)
ğŸš€ Steps to Run the Project
1ï¸âƒ£ Clone the repository:

bash
git clone https://github.com/A-Shenawy/Voice-Sentiment-Analysis.git
cd Voice-Sentiment-Analysis
2ï¸âƒ£ Install dependencies:

bash
pip install -r requirements.txt
3ï¸âƒ£ Run the Jupyter Notebook or Python script for training:

bash
jupyter notebook
# OR
python train_model.py
ğŸ“œ Citation & Credits
Dataset: EYASE Arabic Voice Sentiment Dataset
Author: Ahmed AbdElhamid Shenawy
Technologies Used: Python, TensorFlow, Keras, Librosa, Matplotlib
ğŸŒŸ Contributions & Feedback
Feel free to contribute by opening issues, submitting pull requests, or providing feedback! ğŸš€
