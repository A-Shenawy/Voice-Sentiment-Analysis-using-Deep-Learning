# ğŸ¤ Voice Sentiment Analysis Using Deep Learning  

<img src="https://github.com/A-Shenawy/Voice-Sentiment-Analysis/blob/main/VSA.PNG" width="100%" style="background-size: cover;">

---

## **ğŸ“Œ Project Overview**  
This project aims to classify **emotions from voice recordings** using **deep learning techniques**. Human speech carries rich emotional cues, and analyzing these can enhance **human-computer interaction, mental health monitoring, and customer experience analysis**. The model is trained on the **EYASE dataset**, which contains **Arabic voice samples** labeled with emotions: **happy, sad, angry, and neutral**.  

### **ğŸ¯ Key Objectives**  
âœ… Develop a **CNN-based deep learning model** to classify emotions from speech.  
âœ… Preprocess and **augment** voice data to improve model generalization.  
âœ… Extract **Mel-Spectrogram** features for deep learning-based classification.  
âœ… Evaluate model performance using **accuracy, F1-score, precision, and recall**.  

---

## **ğŸ“Š Dataset Details**  
- **Total audio files**: 579  
- **Sentiment classes**:  
  - **Happy**: 132  
  - **Sad**: 147  
  - **Angry**: 150  
  - **Neutral**: 150  
- **Average sample rate**: **44.1 KHz**  
- **Average audio duration**: **2.33 sec**  
- **Data augmentation techniques applied**: **Noise addition, pitch shifting**  
- **Final dataset after augmentation**: **1,626 audio samples**  

---

## **ğŸ› ï¸ Methodology**  

### **1ï¸âƒ£ Data Preprocessing & Augmentation**  
ğŸ”¹ **Padding & truncating**: Standardizing audio length.  
ğŸ”¹ **Data augmentation**:  
   - **Noise addition** â†’ Simulates real-world background noise.  
   - **Pitch shifting** â†’ Enhances model robustness to voice variations.  

### **2ï¸âƒ£ Feature Extraction**  
ğŸ”¹ **Mel-Spectrogram transformation**: Converts raw audio into spectrogram images.  
ğŸ”¹ **Normalization & down-sampling**: Reducing sample rate for efficient processing.  

### **3ï¸âƒ£ Model Architecture (CNN-Based Approach)**  
ğŸ”¹ **Input**: Mel-Spectrogram images.  
ğŸ”¹ **Convolutional Layers (Conv2D)** â†’ Extracts spatial features.  
ğŸ”¹ **MaxPooling Layers** â†’ Reduces dimensionality.  
ğŸ”¹ **Dropout Layers** â†’ Prevents overfitting.  
ğŸ”¹ **Flatten Layer** â†’ Converts 2D features to 1D.  
ğŸ”¹ **Dense Layers** â†’ Classifies the emotions.  

### **4ï¸âƒ£ Training & Validation**  
ğŸ”¹ **Train-test split**: **80% training, 20% testing**.  
ğŸ”¹ **Cross-validation**: **5-Fold K-Fold validation** to ensure robustness.  

---

## **ğŸ“ˆ Results & Performance**  
ğŸ“Œ **Initial Model Accuracy**: **86.76%**  
ğŸ“Œ **Confusion Matrix Analysis**:  
   - **High precision & recall** for Angry and Sad classes.  
   - **Slight misclassification** in Neutral vs. Happy classes.  

ğŸ“Œ **K-Fold Cross-Validation Scores**:  
   - **Average Accuracy**: **87.7%**  
   - **Average F1 Score**: **87.63%**  
   - **Average Recall**: **87.7%**  
   - **Average Precision**: **87.92%**  

---

## **ğŸ“Œ Future Improvements**  
ğŸ”¹ **Optimize model architecture** for better generalization.  
ğŸ”¹ **Explore transformer-based models** (e.g., Wav2Vec2, Whisper) for speech processing.  
ğŸ”¹ **Integrate emotion detection** with **real-time applications** (e.g., chatbots, virtual assistants).  

---

## **ğŸ“¦ Installation & Usage**  

### **ğŸ”§ Prerequisites**  
Make sure you have the following installed:  
- ğŸ Python 3.x  
- ğŸ¤– TensorFlow / PyTorch  
- ğŸµ Librosa (for audio processing)  
- ğŸ“Š NumPy, Pandas, Matplotlib  
- ğŸ““ Jupyter Notebook  

### **ğŸš€ Steps to Run the Project**  
1ï¸âƒ£ **Clone the repository**  
   ```bash
   git clone https://github.com/A-Shenawy/Voice-Sentiment-Analysis.git
   cd Voice-Sentiment-Analysis
